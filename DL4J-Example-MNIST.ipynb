{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "#Java\n\npackage org.deeplearning4j.examples.feedforward.mnist;\n\nimport org.deeplearning4j.datasets.iterator.impl.MnistDataSetIterator;\nimport org.deeplearning4j.eval.Evaluation;\nimport org.deeplearning4j.nn.conf.MultiLayerConfiguration;\nimport org.deeplearning4j.nn.conf.NeuralNetConfiguration;\nimport org.deeplearning4j.nn.conf.layers.DenseLayer;\nimport org.deeplearning4j.nn.conf.layers.OutputLayer;\nimport org.deeplearning4j.nn.multilayer.MultiLayerNetwork;\nimport org.deeplearning4j.nn.weights.WeightInit;\nimport org.deeplearning4j.optimize.listeners.ScoreIterationListener;\nimport org.nd4j.linalg.activations.Activation;\nimport org.nd4j.linalg.api.ndarray.INDArray;\nimport org.nd4j.linalg.dataset.DataSet;\nimport org.nd4j.linalg.dataset.api.iterator.DataSetIterator;\nimport org.nd4j.linalg.learning.config.Nesterovs;\nimport org.nd4j.linalg.lossfunctions.LossFunctions.LossFunction;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\n\n/**A Simple Multi Layered Perceptron (MLP) applied to digit classification for\n * the MNIST Dataset (http://yann.lecun.com/exdb/mnist/).\n *\n * This file builds one input layer and one hidden layer.\n *\n * The input layer has input dimension of numRows*numColumns where these variables indicate the\n * number of vertical and horizontal pixels in the image. This layer uses a rectified linear unit\n * (relu) activation function. The weights for this layer are initialized by using Xavier initialization\n * (https://prateekvjoshi.com/2016/03/29/understanding-xavier-initialization-in-deep-neural-networks/)\n * to avoid having a steep learning curve. This layer will have 1000 output signals to the hidden layer.\n *\n * The hidden layer has input dimensions of 1000. These are fed from the input layer. The weights\n * for this layer is also initialized using Xavier initialization. The activation function for this\n * layer is a softmax, which normalizes all the 10 outputs such that the normalized sums\n * add up to 1. The highest of these normalized values is picked as the predicted class.\n *\n */\npublic class MLPMnistSingleLayerExample {\n\n    private static Logger log = LoggerFactory.getLogger(MLPMnistSingleLayerExample.class);\n\n    public static void main(String[] args) throws Exception {\n        //number of rows and columns in the input pictures\n        final int numRows = 28;\n        final int numColumns = 28;\n        int outputNum = 10; // number of output classes\n        int batchSize = 128; // batch size for each epoch\n        int rngSeed = 123; // random number seed for reproducibility\n        int numEpochs = 15; // number of epochs to perform\n\n        //Get the DataSetIterators:\n        DataSetIterator mnistTrain = new MnistDataSetIterator(batchSize, true, rngSeed);\n        DataSetIterator mnistTest = new MnistDataSetIterator(batchSize, false, rngSeed);\n\n\n        log.info(\"Build model....\");\n        MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()\n                .seed(rngSeed) //include a random seed for reproducibility\n                // use stochastic gradient descent as an optimization algorithm\n                .updater(new Nesterovs(0.006, 0.9))\n                .l2(1e-4)\n                .list()\n                .layer(0, new DenseLayer.Builder() //create the first, input layer with xavier initialization\n                        .nIn(numRows * numColumns)\n                        .nOut(1000)\n                        .activation(Activation.RELU)\n                        .weightInit(WeightInit.XAVIER)\n                        .build())\n                .layer(1, new OutputLayer.Builder(LossFunction.NEGATIVELOGLIKELIHOOD) //create hidden layer\n                        .nIn(1000)\n                        .nOut(outputNum)\n                        .activation(Activation.SOFTMAX)\n                        .weightInit(WeightInit.XAVIER)\n                        .build())\n                .pretrain(false).backprop(true) //use backpropagation to adjust weights\n                .build();\n\n        MultiLayerNetwork model = new MultiLayerNetwork(conf);\n        model.init();\n        //print the score with every 1 iteration\n        model.setListeners(new ScoreIterationListener(1));\n\n        log.info(\"Train model....\");\n        for( int i=0; i<numEpochs; i++ ){\n            model.fit(mnistTrain);\n        }\n\n\n        log.info(\"Evaluate model....\");\n        Evaluation eval = new Evaluation(outputNum); //create an evaluation object with 10 possible classes\n        while(mnistTest.hasNext()){\n            DataSet next = mnistTest.next();\n            INDArray output = model.output(next.getFeatureMatrix()); //get the networks prediction\n            eval.eval(next.getLabels(), output); //check the prediction against the true class\n        }\n\n        log.info(eval.stats());\n        log.info(\"****************Example finished********************\");\n\n    }\n\n}", 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 with Spark 2.1", 
            "name": "python3-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "pygments_lexer": "ipython3", 
            "file_extension": ".py", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}