{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "!pip install http://download.pytorch.org/whl/cpu/torch-0.3.1-cp27-cp27mu-linux_x86_64.whl\n!pip install torchvision", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "import torch\nimport torch.autograd as autograd\nimport torch.nn as nn\nimport torch.optim as optim\n\ntorch.manual_seed(123)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "<torch._C.Generator at 0x7f36c65623d0>"
                    }, 
                    "execution_count": 9, 
                    "metadata": {}
                }
            ], 
            "execution_count": 9
        }, 
        {
            "source": "print(torch.Tensor([1,2,3]))\nprint(torch.Tensor([[1,2,3],[2,3,4]]))\nprint(torch.Tensor([[[1,2,3],[3,4,5]],\n                   [[4,5,6],[6,7,8]]]))", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "\n 1\n 2\n 3\n[torch.FloatTensor of size 3]\n\n\n 1  2  3\n 2  3  4\n[torch.FloatTensor of size 2x3]\n\n\n(0 ,.,.) = \n  1  2  3\n  3  4  5\n\n(1 ,.,.) = \n  4  5  6\n  6  7  8\n[torch.FloatTensor of size 2x2x3]\n\n"
                }
            ], 
            "execution_count": 14
        }, 
        {
            "source": "t1 = torch.randn(60000, 3, 28, 28)\nt1_reshaped = t1.view(32, -1, 3, 28, 28)\nprint(t1_reshaped.shape)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "torch.Size([32, 1875, 3, 28, 28])\n"
                }
            ], 
            "execution_count": 17
        }, 
        {
            "source": "x = autograd.Variable(torch.Tensor([1,2,3]), requires_grad=True)\nprint(x.data)\n\ny = autograd.Variable(torch.Tensor([4,5,6]), requires_grad=True)\nprint(y.data)\n\nz = x + y\nprint(z)\n\nprint(z.grad_fn)\n\ns = z.sum()\nprint(s)\nprint(s.grad_fn)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "\n 1\n 2\n 3\n[torch.FloatTensor of size 3]\n\n\n 4\n 5\n 6\n[torch.FloatTensor of size 3]\n\nVariable containing:\n 5\n 7\n 9\n[torch.FloatTensor of size 3]\n\n<AddBackward1 object at 0x7f36c6522f90>\nVariable containing:\n 21\n[torch.FloatTensor of size 1]\n\n<SumBackward0 object at 0x7f36aad427d0>\n"
                }
            ], 
            "execution_count": 21
        }, 
        {
            "source": "s.backward(retain_graph=True)\n\nprint(x)\nprint(x.grad)\nprint(y.grad)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Variable containing:\n 1\n 2\n 3\n[torch.FloatTensor of size 3]\n\nVariable containing:\n 7\n 7\n 7\n[torch.FloatTensor of size 3]\n\nVariable containing:\n 7\n 7\n 7\n[torch.FloatTensor of size 3]\n\n"
                }
            ], 
            "execution_count": 28
        }, 
        {
            "source": "x = torch.randn((2,2))\ny = torch.randn((2,2))\nz = x + y\n\nprint(z)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "\n 0.7758  1.4609\n-4.4736 -1.3541\n[torch.FloatTensor of size 2x2]\n\n"
                }
            ], 
            "execution_count": 30
        }, 
        {
            "source": "_x = autograd.Variable(x, requires_grad=True)\n_y = autograd.Variable(y, requires_grad=True)\n_z = _x + _y\n\nprint(_z.grad_fn)\n\nd_z = _z.data\nprint(d_z)\nnew_d_z = autograd.Variable(d_z, requires_grad=True)\nprint(new_d_z.grad_fn)\n\nnew_d_z.backward(retain_graph=True)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "<AddBackward1 object at 0x7f36aacb8110>\n\n 0.7758  1.4609\n-4.4736 -1.3541\n[torch.FloatTensor of size 2x2]\n\nNone\n"
                }, 
                {
                    "output_type": "error", 
                    "evalue": "grad can be implicitly created only for scalar outputs", 
                    "traceback": [
                        "\u001b[0;31m\u001b[0m", 
                        "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)", 
                        "\u001b[0;32m<ipython-input-37-9b5e1d32564b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_d_z\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mnew_d_z\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", 
                        "\u001b[0;32m/gpfs/fs01/user/s613-ce8dfc36d194a2-56b67737d6cd/.local/lib/python2.7/site-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/gpfs/fs01/user/s613-ce8dfc36d194a2-56b67737d6cd/.local/lib/python2.7/site-packages/torch/autograd/__init__.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mgrad_variables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0mgrad_variables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_variables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_variables\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/gpfs/fs01/user/s613-ce8dfc36d194a2-56b67737d6cd/.local/lib/python2.7/site-packages/torch/autograd/__init__.pyc\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, user_create_graph)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 new_grads.append(\n", 
                        "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
                    ], 
                    "ename": "RuntimeError"
                }
            ], 
            "execution_count": 37
        }, 
        {
            "source": "print(torch.cuda.is_available())\nif torch.cuda.is_available():\n    a = torch.LongTensor(10).fill_(3).cuda()\n    print(type(a))\n    b = a.cpu()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "False\n"
                }
            ], 
            "execution_count": 39
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 2 with Spark 2.1", 
            "name": "python2-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "2.7.11", 
            "name": "python", 
            "pygments_lexer": "ipython2", 
            "file_extension": ".py", 
            "codemirror_mode": {
                "version": 2, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}