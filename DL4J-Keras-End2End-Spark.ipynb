{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "!wget https://github.com/maxpumperla/dl4j_coursera/releases/download/v0.4/dl4j-snapshot.jar", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "--2018-04-29 23:52:24--  https://github.com/maxpumperla/dl4j_coursera/releases/download/v0.4/dl4j-snapshot.jar\nResolving github.com (github.com)... 192.30.253.112, 192.30.253.113\nConnecting to github.com (github.com)|192.30.253.112|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://github-production-release-asset-2e65be.s3.amazonaws.com/113966420/3472050e-f84b-11e7-90f0-d69fe0bedce0?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20180430%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20180430T045225Z&X-Amz-Expires=300&X-Amz-Signature=d29c4f7a8703c8b32a1b0b7793a7a4b06a66ad3d7f9f08d32fac2df6e74b993e&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Ddl4j-snapshot.jar&response-content-type=application%2Foctet-stream [following]\n--2018-04-29 23:52:25--  https://github-production-release-asset-2e65be.s3.amazonaws.com/113966420/3472050e-f84b-11e7-90f0-d69fe0bedce0?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20180430%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20180430T045225Z&X-Amz-Expires=300&X-Amz-Signature=d29c4f7a8703c8b32a1b0b7793a7a4b06a66ad3d7f9f08d32fac2df6e74b993e&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Ddl4j-snapshot.jar&response-content-type=application%2Foctet-stream\nResolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.216.227.48\nConnecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.216.227.48|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 486534267 (464M) [application/octet-stream]\nSaving to: \u2018dl4j-snapshot.jar\u2019\n\n100%[======================================>] 486,534,267 30.9MB/s   in 13s    \n\n2018-04-29 23:52:38 (35.4 MB/s) - \u2018dl4j-snapshot.jar\u2019 saved [486534267/486534267]\n\n"
                }
            ], 
            "execution_count": 1
        }, 
        {
            "source": "!wget https://raw.githubusercontent.com/maxpumperla/dl4j_coursera/master/iris.txt", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "--2018-04-29 23:53:55--  https://raw.githubusercontent.com/maxpumperla/dl4j_coursera/master/iris.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.48.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.48.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2850 (2.8K) [text/plain]\nSaving to: \u2018iris.txt.1\u2019\n\n100%[======================================>] 2,850       --.-K/s   in 0s      \n\n2018-04-29 23:53:55 (28.9 MB/s) - \u2018iris.txt.1\u2019 saved [2850/2850]\n\n"
                }
            ], 
            "execution_count": 2
        }, 
        {
            "source": "\nimport numpy\nimport pandas\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.utils import np_utils\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import Pipeline", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stderr", 
                    "text": "Using TensorFlow backend.\n/usr/local/src/conda3_runtime/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\n"
                }
            ], 
            "execution_count": 3
        }, 
        {
            "source": "\n# fix random seed for reproducibility\nseed = 10\nnumpy.random.seed(seed)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 4
        }, 
        {
            "source": "# load dataset\ndataframe = pandas.read_csv(\"iris.txt\", header=None)\ndataset = dataframe.values\nX = dataset[:,0:4].astype(float)\nY = dataset[:,4]", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 5
        }, 
        {
            "source": "print(X.shape)\nprint(Y.shape)\nprint(Y)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "(150, 4)\n(150,)\n[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n 2. 2. 2. 2. 2. 2.]\n"
                }
            ], 
            "execution_count": 10
        }, 
        {
            "source": "encoder = LabelEncoder()\nencoder.fit(Y)\nencoded_Y = encoder.transform(Y)\n# convert integers to dummy variables (i.e. one hot encoded)\ndummy_y = np_utils.to_categorical(encoded_Y)\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 6
        }, 
        {
            "source": "print(encoded_Y.shape)\nprint(dummy_y.shape)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "(150,)\n(150, 3)\n(150, 3)\n"
                }
            ], 
            "execution_count": 14
        }, 
        {
            "source": "model = Sequential()\nmodel.add(Dense(4, input_dim=4, activation='relu'))\nmodel.add(Dense(3, activation='sigmoid'))\n# Compile model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.fit(X,dummy_y,epochs=100,batch_size=5)\nmodel.save('iris_model.h5')", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Epoch 1/100\n150/150 [==============================] - 0s - loss: 1.1057 - acc: 0.1933     \nEpoch 2/100\n150/150 [==============================] - 0s - loss: 1.1005 - acc: 0.2933     \nEpoch 3/100\n150/150 [==============================] - 0s - loss: 1.0997 - acc: 0.3133     \nEpoch 4/100\n150/150 [==============================] - 0s - loss: 1.0993 - acc: 0.3133     \nEpoch 5/100\n150/150 [==============================] - 0s - loss: 1.0990 - acc: 0.2600     \nEpoch 6/100\n150/150 [==============================] - 0s - loss: 1.0988 - acc: 0.3200     \nEpoch 7/100\n150/150 [==============================] - 0s - loss: 1.0988 - acc: 0.3267     \nEpoch 8/100\n150/150 [==============================] - 0s - loss: 1.0988 - acc: 0.3333     \nEpoch 9/100\n150/150 [==============================] - 0s - loss: 1.0986 - acc: 0.3333     \nEpoch 10/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.3333     \nEpoch 11/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.3333     \nEpoch 12/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.3333     \nEpoch 13/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.3333     \nEpoch 14/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.3333     \nEpoch 15/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.3333     \nEpoch 16/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.3333     \nEpoch 17/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.3333     \nEpoch 18/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.3333         \nEpoch 19/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.3333     \nEpoch 20/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.2667     \nEpoch 21/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.3267     \nEpoch 22/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.3333     \nEpoch 23/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.3333     \nEpoch 24/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.3333     \nEpoch 25/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.3333     \nEpoch 26/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.2933     \nEpoch 27/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.3333     \nEpoch 28/100\n150/150 [==============================] - 0s - loss: 1.0986 - acc: 0.3333     \nEpoch 29/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.2800     \nEpoch 30/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.3333     \nEpoch 31/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.2733     \nEpoch 32/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.2867     \nEpoch 33/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.3067     \nEpoch 34/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.2400     \nEpoch 35/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.3067         \nEpoch 36/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.2933     \nEpoch 37/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.3333         \nEpoch 38/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.3000     \nEpoch 39/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.3133         \nEpoch 40/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.2933     \nEpoch 41/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.3333     \nEpoch 42/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.2467     \nEpoch 43/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.2667     \nEpoch 44/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.3333     \nEpoch 45/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.3333     \nEpoch 46/100\n150/150 [==============================] - 0s - loss: 1.0988 - acc: 0.3333         \nEpoch 47/100\n150/150 [==============================] - 0s - loss: 1.0986 - acc: 0.3333     \nEpoch 48/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.2600         \nEpoch 49/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.2867         \nEpoch 50/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.2733         \nEpoch 51/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.2867     \nEpoch 52/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.2400     \nEpoch 53/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.3000     \nEpoch 54/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.3333     \nEpoch 55/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.2933     \nEpoch 56/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.2867         \nEpoch 57/100\n150/150 [==============================] - 0s - loss: 1.0986 - acc: 0.3333     \nEpoch 58/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.3333     \nEpoch 59/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.2600     \nEpoch 60/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.3200     \nEpoch 61/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.2733     \nEpoch 62/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.3267     \nEpoch 63/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.2933     \nEpoch 64/100\n150/150 [==============================] - 0s - loss: 1.0988 - acc: 0.2667         \nEpoch 65/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.3200     \nEpoch 66/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.3133     \nEpoch 67/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.2733     \nEpoch 68/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.3200     \nEpoch 69/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.2800     \nEpoch 70/100\n150/150 [==============================] - 0s - loss: 1.0989 - acc: 0.2867         \nEpoch 71/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.2733         \nEpoch 72/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.3000     \nEpoch 73/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.2733     \nEpoch 74/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.3200     \nEpoch 75/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.2800     \nEpoch 76/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.3333     \nEpoch 77/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.2933     \nEpoch 78/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.2867     \nEpoch 79/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.3067     \nEpoch 80/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.3067     \nEpoch 81/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.2800     \nEpoch 82/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.2867     \nEpoch 83/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.3067         \nEpoch 84/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.2667         \nEpoch 85/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.3067         \nEpoch 86/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.2933     \nEpoch 87/100\n150/150 [==============================] - 0s - loss: 1.0986 - acc: 0.3333     \nEpoch 88/100\n"
                }, 
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.3000     \nEpoch 89/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.3333     \nEpoch 90/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.2867     \nEpoch 91/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.3133     \nEpoch 92/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.2933     \nEpoch 93/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.3133     \nEpoch 94/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.2400     \nEpoch 95/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.3000     \nEpoch 96/100\n150/150 [==============================] - 0s - loss: 1.0988 - acc: 0.3067     \nEpoch 97/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.3333     \nEpoch 98/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.2933     \nEpoch 99/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.2667     \nEpoch 100/100\n150/150 [==============================] - 0s - loss: 1.0987 - acc: 0.2800     \n"
                }
            ], 
            "execution_count": 19
        }, 
        {
            "source": "#some learners constantly reported 502 errors in Watson Studio. \n#This is due to the limited resources in the free tier and the heavy resource consumption of Keras.\n#This is a workaround to limit resource consumption\n\nfrom keras import backend as K\n\nK.set_session(K.tf.Session(config=K.tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)))\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 16
        }, 
        {
            "source": "!$SPARK_HOME/bin/spark-submit \\\n--class skymind.dsx.KerasImportCSVSparkRunner \\\n--files iris.txt,iris_model.h5 \\\n--master $MASTER \\\ndl4j-snapshot.jar \\\n-batchSizePerWorker 15 \\\n-indexLabel 4 \\\n-train false \\\n-numClasses 3 \\\n-modelFileName iris_model.h5 \\\n-dataFileName iris.txt", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "SLF4J: Class path contains multiple SLF4J bindings.\nSLF4J: Found binding in [jar:file:/usr/local/src/spark21master/spark-2.1.2-bin-2.7.3/jars/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/usr/local/src/wml-libs.v23/spark-2.0/jars/tika-app-2.0-1.14.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/usr/local/src/wml-libs.v23/spark-2.0/jars/ml-event-client-scala-library-0.1.55-201709150512-allinone.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/usr/local/src/dataconnector-dw-2.0/spark-2.0.0/Server/connectivity/thirdparty/slf4j-simple-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\nSLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n18/04/30 00:58:19 INFO apache.spark.SparkContext: Running Spark version 2.1.2\n18/04/30 00:58:20 WARN hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n18/04/30 00:58:20 INFO apache.spark.SparkContext: Spark configuration:\nspark.app.name=DL4J Keras model import runner\nspark.authenticate=true\nspark.authenticate.enableSaslEncryption=true\nspark.authenticate.secret=.secret\nspark.deploy.resourceScheduler.factory=org.apache.spark.deploy.master.EGOResourceSchedulerFactory\nspark.driver.maxResultSize=1210M\nspark.driver.memory=1512M\nspark.ego.authenticate.tenantSecret.filename=.secret\nspark.ego.authenticate.tenantSecret.pathPrefix=/gpfs/fs01/user\nspark.eventLog.dir=/gpfs/fs01/user/s613-ce8dfc36d194a2-56b67737d6cd/events\nspark.eventLog.enabled=true\nspark.executor.extraJavaOptions=-Djava.security.egd=file:/dev/./urandom\nspark.executor.memory=6G\nspark.extraListeners=com.ibm.spaas.listeners.DB2DialectRegistrar\nspark.files=file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s613-ce8dfc36d194a2-56b67737d6cd/notebook/work/iris.txt,file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s613-ce8dfc36d194a2-56b67737d6cd/notebook/work/iris_model.h5\nspark.history.fs.logDirectory=/gpfs/fs01/user/s613-ce8dfc36d194a2-56b67737d6cd/events\nspark.io.encryption.enabled=true\nspark.jars=file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s613-ce8dfc36d194a2-56b67737d6cd/notebook/work/dl4j-snapshot.jar\nspark.logConf=true\nspark.master=spark://yp-spark-dal09-env5-0027:7088\nspark.network.sasl.serverAlwaysEncrypt=true\nspark.port.maxRetries=512\nspark.r.command=/usr/local/src/bluemix_jupyter_bundle.v90/R/bin/Rscript\nspark.shuffle.service.enabled=true\nspark.shuffle.service.port=7342\nspark.sql.ui.retainedExecutions=0\nspark.submit.deployMode=client\nspark.task.maxFailures=10\nspark.ui.enabled=false\nspark.ui.retainedJobs=0\nspark.ui.retainedStages=0\nspark.worker.ui.retainedExecutors=0\n18/04/30 00:58:21 INFO apache.spark.SecurityManager: Changing view acls to: s613-ce8dfc36d194a2-56b67737d6cd\n18/04/30 00:58:21 INFO apache.spark.SecurityManager: Changing modify acls to: s613-ce8dfc36d194a2-56b67737d6cd\n18/04/30 00:58:21 INFO apache.spark.SecurityManager: Changing view acls groups to: \n18/04/30 00:58:21 INFO apache.spark.SecurityManager: Changing modify acls groups to: \n18/04/30 00:58:21 INFO apache.spark.SecurityManager: SecurityManager: authentication enabled; ui acls disabled; users  with view permissions: Set(s613-ce8dfc36d194a2-56b67737d6cd); groups with view permissions: Set(); users  with modify permissions: Set(s613-ce8dfc36d194a2-56b67737d6cd); groups with modify permissions: Set()\n18/04/30 00:58:21 INFO spark.util.Utils: Successfully started service 'sparkDriver' on port 45622.\n18/04/30 00:58:21 INFO apache.spark.SparkEnv: Registering MapOutputTracker\n18/04/30 00:58:21 INFO apache.spark.SparkEnv: Registering BlockManagerMaster\n18/04/30 00:58:21 INFO spark.storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n18/04/30 00:58:21 INFO spark.storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n18/04/30 00:58:21 INFO storage.memory.MemoryStore: MemoryStore started with capacity 727.2 MB\n18/04/30 00:58:21 INFO apache.spark.SparkEnv: Registering OutputCommitCoordinator\n18/04/30 00:58:21 INFO apache.spark.SparkContext: Added JAR file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s613-ce8dfc36d194a2-56b67737d6cd/notebook/work/dl4j-snapshot.jar at spark://10.143.133.233:45622/jars/dl4j-snapshot.jar with timestamp 1525067901480\n18/04/30 00:58:21 INFO apache.spark.SparkContext: Added file file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s613-ce8dfc36d194a2-56b67737d6cd/notebook/work/iris.txt at spark://10.143.133.233:45622/files/iris.txt with timestamp 1525067901683\n18/04/30 00:58:21 INFO spark.util.Utils: Copying /gpfs/global_fs01/sym_shared/YPProdSpark/user/s613-ce8dfc36d194a2-56b67737d6cd/notebook/work/iris.txt to /tmp/spark-21-ego-master/work/spark-efda511b-65f4-40ed-b11b-21ccc57aac51/userFiles-683df8a9-edcd-4761-be0e-5f11356855e5/iris.txt\n18/04/30 00:58:21 INFO apache.spark.SparkContext: Added file file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s613-ce8dfc36d194a2-56b67737d6cd/notebook/work/iris_model.h5 at spark://10.143.133.233:45622/files/iris_model.h5 with timestamp 1525067901696\n18/04/30 00:58:21 INFO spark.util.Utils: Copying /gpfs/global_fs01/sym_shared/YPProdSpark/user/s613-ce8dfc36d194a2-56b67737d6cd/notebook/work/iris_model.h5 to /tmp/spark-21-ego-master/work/spark-efda511b-65f4-40ed-b11b-21ccc57aac51/userFiles-683df8a9-edcd-4761-be0e-5f11356855e5/iris_model.h5\n18/04/30 00:58:21 INFO spark.util.EGOSparkDockerConfig: Executor Container Type is 'normal' from local configuration file.\n18/04/30 00:58:21 INFO spark.util.EGOSparkDockerConfig: Driver Container Type is 'normal' from local configuration file.\n18/04/30 00:58:21 INFO spark.util.EGOSparkDockerConfig: spark-ego-docker.conf will not be parsed as docker is not defined as any container type.\n18/04/30 00:58:21 INFO cluster.ego.EGOFineGrainedSchedulerBackend: setting reserve=0, priority=1, limit=2147483647, gpuLimit=2147483647, master=[Ljava.lang.String;@2da009f3\n18/04/30 00:58:21 INFO client.ego.EGOAppClient$ClientEndpoint: Connecting to master spark://yp-spark-dal09-env5-0027:7088...\n18/04/30 00:58:21 INFO network.client.TransportClientFactory: Successfully created connection to yp-spark-dal09-env5-0027/10.143.133.233:7088 after 140 ms (111 ms spent in bootstraps)\n18/04/30 00:58:22 INFO cluster.ego.EGOFineGrainedSchedulerBackend: Connected to Spark cluster with app ID app-20180430005822-0082-f163ea89-e12a-495a-ae74-10361283fcb5\n18/04/30 00:58:22 INFO cluster.ego.EGOFineGrainedSchedulerBackend: Application registered successfully as app-20180430005822-0082-f163ea89-e12a-495a-ae74-10361283fcb5, executor Container type is normal\n18/04/30 00:58:22 INFO spark.storage.DiskBlockManager: Init the driver local dir\n18/04/30 00:58:22 INFO spark.storage.DiskBlockManager: Created local directory at /tmp/spark-21-ego-master/work/blockmgr-7ea5ee04-53f4-426f-a44a-4c4835fdacb1\n18/04/30 00:58:22 INFO spark.util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34826.\n18/04/30 00:58:22 INFO network.netty.NettyBlockTransferService: Server created on 10.143.133.233:34826\n18/04/30 00:58:22 INFO spark.storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n18/04/30 00:58:22 INFO spark.storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.143.133.233, 34826, None)\n18/04/30 00:58:22 INFO spark.storage.BlockManagerMasterEndpoint: Registering block manager 10.143.133.233:34826 with 727.2 MB RAM, BlockManagerId(driver, 10.143.133.233, 34826, None)\n18/04/30 00:58:22 INFO spark.storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.143.133.233, 34826, None)\n18/04/30 00:58:22 INFO spark.storage.BlockManager: external shuffle service port = 7342\n18/04/30 00:58:22 INFO spark.storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.143.133.233, 34826, None)\n18/04/30 00:58:22 INFO jetty.util.log: Logging initialized @3523ms\n18/04/30 00:58:22 INFO spark.scheduler.EventLoggingListener: Logging events to file:/gpfs/fs01/user/s613-ce8dfc36d194a2-56b67737d6cd/events/app-20180430005822-0082-f163ea89-e12a-495a-ae74-10361283fcb5\n18/04/30 00:58:22 INFO apache.spark.SparkContext: Registered listener com.ibm.spaas.listeners.DB2DialectRegistrar\n18/04/30 00:58:22 INFO cluster.ego.EGODeployScheduler: Spark context initialized.\n"
                }, 
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "18/04/30 00:58:22 INFO root: application started with appid Some(app-20180430005822-0082-f163ea89-e12a-495a-ae74-10361283fcb5) and app name DL4J Keras model import runner and application start time is 1525067899765\n18/04/30 00:58:23 WARN keras.utils.KerasModelUtils: Could not read keras backend used (no backend field found) \n\n18/04/30 00:58:23 WARN keras.utils.KerasModelUtils: Unable to match layer parameter name bias:0 for stored weights.\n18/04/30 00:58:23 INFO linalg.factory.Nd4jBackend: Loaded [CpuBackend] backend\n18/04/30 00:58:23 WARN org.reflections.Reflections: given scan urls are empty. set urls in the configuration\n18/04/30 00:58:23 INFO nd4j.nativeblas.NativeOpsHolder: Number of threads used for NativeOps: 12\n18/04/30 00:58:23 INFO nd4j.nativeblas.Nd4jBlas: Number of threads used for BLAS: 12\n18/04/30 00:58:23 INFO ops.executioner.DefaultOpExecutioner: Backend used: [CPU]; OS: [Linux]\n18/04/30 00:58:23 INFO ops.executioner.DefaultOpExecutioner: Cores: [48]; Memory: [1.5GB];\n18/04/30 00:58:23 INFO ops.executioner.DefaultOpExecutioner: Blas vendor: [MKL]\n18/04/30 00:58:24 INFO org.reflections.Reflections: Reflections took 808 ms to scan 1 urls, producing 495 keys and 2499 values \n18/04/30 00:58:24 WARN keras.utils.KerasModelUtils: Unable to match layer parameter name kernel:0 for stored weights.\n18/04/30 00:58:24 WARN keras.utils.KerasModelUtils: Unable to match layer parameter name bias:0 for stored weights.\n18/04/30 00:58:24 WARN keras.utils.KerasModelUtils: Unable to match layer parameter name kernel:0 for stored weights.\n18/04/30 00:58:24 WARN modelimport.keras.KerasSequentialModel: Model cannot be trained: output layer dense_4 is not an IOutputLayer (no loss function specified)\n18/04/30 00:58:25 WARN org.reflections.Reflections: could not create Dir using jarFile from url file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s613-ce8dfc36d194a2-56b67737d6cd/data/libs/*.jar. skipping.\njava.lang.NullPointerException\n\tat java.util.zip.ZipFile.<init>(ZipFile.java:223)\n\tat java.util.zip.ZipFile.<init>(ZipFile.java:165)\n\tat java.util.jar.JarFile.<init>(JarFile.java:179)\n\tat java.util.jar.JarFile.<init>(JarFile.java:143)\n\tat org.reflections.vfs.Vfs$DefaultUrlTypes$1.createDir(Vfs.java:212)\n\tat org.reflections.vfs.Vfs.fromURL(Vfs.java:99)\n\tat org.reflections.vfs.Vfs.fromURL(Vfs.java:91)\n\tat org.reflections.Reflections.scan(Reflections.java:237)\n\tat org.reflections.Reflections.scan(Reflections.java:204)\n\tat org.reflections.Reflections.<init>(Reflections.java:129)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.registerSubtypes(NeuralNetConfiguration.java:507)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.configureMapper(NeuralNetConfiguration.java:462)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.initMapper(NeuralNetConfiguration.java:435)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.<clinit>(NeuralNetConfiguration.java:122)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration$Builder.build(NeuralNetConfiguration.java:1045)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration$ListBuilder.build(NeuralNetConfiguration.java:290)\n\tat org.deeplearning4j.nn.modelimport.keras.KerasSequentialModel.getMultiLayerConfiguration(KerasSequentialModel.java:203)\n\tat org.deeplearning4j.nn.modelimport.keras.KerasSequentialModel.getMultiLayerNetwork(KerasSequentialModel.java:223)\n\tat org.deeplearning4j.nn.modelimport.keras.KerasSequentialModel.getMultiLayerNetwork(KerasSequentialModel.java:213)\n\tat org.deeplearning4j.nn.modelimport.keras.KerasModelImport.importKerasSequentialModelAndWeights(KerasModelImport.java:143)\n\tat skymind.dsx.KerasImportCSVSparkRunner.entryPoint(KerasImportCSVSparkRunner.java:102)\n\tat skymind.dsx.KerasImportCSVSparkRunner.main(KerasImportCSVSparkRunner.java:80)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\n\tat java.lang.reflect.Method.invoke(Method.java:507)\n\tat org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:819)\n\tat org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:196)\n\tat org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:221)\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:129)\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n18/04/30 00:58:25 WARN org.reflections.Reflections: could not create Vfs.Dir from url. ignoring the exception and continuing\norg.reflections.ReflectionsException: Could not open url connection\n\tat org.reflections.vfs.JarInputDir$1$1.<init>(JarInputDir.java:37)\n\tat org.reflections.vfs.JarInputDir$1.iterator(JarInputDir.java:33)\n\tat org.reflections.Reflections.scan(Reflections.java:240)\n\tat org.reflections.Reflections.scan(Reflections.java:204)\n\tat org.reflections.Reflections.<init>(Reflections.java:129)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.registerSubtypes(NeuralNetConfiguration.java:507)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.configureMapper(NeuralNetConfiguration.java:462)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.initMapper(NeuralNetConfiguration.java:435)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.<clinit>(NeuralNetConfiguration.java:122)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration$Builder.build(NeuralNetConfiguration.java:1045)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration$ListBuilder.build(NeuralNetConfiguration.java:290)\n\tat org.deeplearning4j.nn.modelimport.keras.KerasSequentialModel.getMultiLayerConfiguration(KerasSequentialModel.java:203)\n\tat org.deeplearning4j.nn.modelimport.keras.KerasSequentialModel.getMultiLayerNetwork(KerasSequentialModel.java:223)\n\tat org.deeplearning4j.nn.modelimport.keras.KerasSequentialModel.getMultiLayerNetwork(KerasSequentialModel.java:213)\n\tat org.deeplearning4j.nn.modelimport.keras.KerasModelImport.importKerasSequentialModelAndWeights(KerasModelImport.java:143)\n\tat skymind.dsx.KerasImportCSVSparkRunner.entryPoint(KerasImportCSVSparkRunner.java:102)\n\tat skymind.dsx.KerasImportCSVSparkRunner.main(KerasImportCSVSparkRunner.java:80)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\n\tat java.lang.reflect.Method.invoke(Method.java:507)\n\tat org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:819)\n\tat org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:196)\n\tat org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:221)\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:129)\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\nCaused by: java.io.FileNotFoundException: /gpfs/global_fs01/sym_shared/YPProdSpark/user/s613-ce8dfc36d194a2-56b67737d6cd/data/libs/*.jar (No such file or directory)\n\tat java.io.FileInputStream.open(FileInputStream.java:212)\n\tat java.io.FileInputStream.<init>(FileInputStream.java:152)\n\tat java.io.FileInputStream.<init>(FileInputStream.java:104)\n\tat sun.net.www.protocol.file.FileURLConnection.connect(FileURLConnection.java:103)\n\tat sun.net.www.protocol.file.FileURLConnection.getInputStream(FileURLConnection.java:201)\n\tat org.reflections.vfs.JarInputDir$1$1.<init>(JarInputDir.java:36)\n\t... 25 more\n18/04/30 00:58:25 WARN org.reflections.Reflections: could not create Dir using directory from url file:/usr/local/src/dataconnector-dw-2.0/spark-2.0.0/Server/connectivity/FaspStreamSDK/lib/*. skipping.\njava.lang.NullPointerException\n\tat org.reflections.vfs.Vfs$DefaultUrlTypes$3.matches(Vfs.java:239)\n\tat org.reflections.vfs.Vfs.fromURL(Vfs.java:98)\n\tat org.reflections.vfs.Vfs.fromURL(Vfs.java:91)\n\tat org.reflections.Reflections.scan(Reflections.java:237)\n\tat org.reflections.Reflections.scan(Reflections.java:204)\n\tat org.reflections.Reflections.<init>(Reflections.java:129)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.registerSubtypes(NeuralNetConfiguration.java:507)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.configureMapper(NeuralNetConfiguration.java:462)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.initMapper(NeuralNetConfiguration.java:435)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.<clinit>(NeuralNetConfiguration.java:122)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration$Builder.build(NeuralNetConfiguration.java:1045)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration$ListBuilder.build(NeuralNetConfiguration.java:290)\n\tat org.deeplearning4j.nn.modelimport.keras.KerasSequentialModel.getMultiLayerConfiguration(KerasSequentialModel.java:203)\n\tat org.deeplearning4j.nn.modelimport.keras.KerasSequentialModel.getMultiLayerNetwork(KerasSequentialModel.java:223)\n\tat org.deeplearning4j.nn.modelimport.keras.KerasSequentialModel.getMultiLayerNetwork(KerasSequentialModel.java:213)\n\tat org.deeplearning4j.nn.modelimport.keras.KerasModelImport.importKerasSequentialModelAndWeights(KerasModelImport.java:143)\n\tat skymind.dsx.KerasImportCSVSparkRunner.entryPoint(KerasImportCSVSparkRunner.java:102)\n\tat skymind.dsx.KerasImportCSVSparkRunner.main(KerasImportCSVSparkRunner.java:80)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\n\tat java.lang.reflect.Method.invoke(Method.java:507)\n\tat org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:819)\n\tat org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:196)\n\tat org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:221)\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:129)\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n18/04/30 00:58:25 WARN org.reflections.Reflections: could not create Vfs.Dir from url. ignoring the exception and continuing\norg.reflections.ReflectionsException: could not create Vfs.Dir from url, no matching UrlType was found [file:/usr/local/src/dataconnector-dw-2.0/spark-2.0.0/Server/connectivity/FaspStreamSDK/lib/*]\neither use fromURL(final URL url, final List<UrlType> urlTypes) or use the static setDefaultURLTypes(final List<UrlType> urlTypes) or addDefaultURLTypes(UrlType urlType) with your specialized UrlType.\n\tat org.reflections.vfs.Vfs.fromURL(Vfs.java:109)\n\tat org.reflections.vfs.Vfs.fromURL(Vfs.java:91)\n\tat org.reflections.Reflections.scan(Reflections.java:237)\n\tat org.reflections.Reflections.scan(Reflections.java:204)\n\tat org.reflections.Reflections.<init>(Reflections.java:129)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.registerSubtypes(NeuralNetConfiguration.java:507)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.configureMapper(NeuralNetConfiguration.java:462)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.initMapper(NeuralNetConfiguration.java:435)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.<clinit>(NeuralNetConfiguration.java:122)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration$Builder.build(NeuralNetConfiguration.java:1045)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration$ListBuilder.build(NeuralNetConfiguration.java:290)\n\tat org.deeplearning4j.nn.modelimport.keras.KerasSequentialModel.getMultiLayerConfiguration(KerasSequentialModel.java:203)\n\tat org.deeplearning4j.nn.modelimport.keras.KerasSequentialModel.getMultiLayerNetwork(KerasSequentialModel.java:223)\n\tat org.deeplearning4j.nn.modelimport.keras.KerasSequentialModel.getMultiLayerNetwork(KerasSequentialModel.java:213)\n\tat org.deeplearning4j.nn.modelimport.keras.KerasModelImport.importKerasSequentialModelAndWeights(KerasModelImport.java:143)\n\tat skymind.dsx.KerasImportCSVSparkRunner.entryPoint(KerasImportCSVSparkRunner.java:102)\n\tat skymind.dsx.KerasImportCSVSparkRunner.main(KerasImportCSVSparkRunner.java:80)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\n\tat java.lang.reflect.Method.invoke(Method.java:507)\n\tat org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:819)\n\tat org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:196)\n\tat org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:221)\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:129)\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n"
                }, 
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "18/04/30 00:58:34 WARN org.reflections.Reflections: could not create Dir using directory from url file:/usr/local/src/dataconnector-dw-2.0/spark-2.0.0/Server/connectivity/connectors/*/*. skipping.\njava.lang.NullPointerException\n\tat org.reflections.vfs.Vfs$DefaultUrlTypes$3.matches(Vfs.java:239)\n\tat org.reflections.vfs.Vfs.fromURL(Vfs.java:98)\n\tat org.reflections.vfs.Vfs.fromURL(Vfs.java:91)\n\tat org.reflections.Reflections.scan(Reflections.java:237)\n\tat org.reflections.Reflections.scan(Reflections.java:204)\n\tat org.reflections.Reflections.<init>(Reflections.java:129)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.registerSubtypes(NeuralNetConfiguration.java:507)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.configureMapper(NeuralNetConfiguration.java:462)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.initMapper(NeuralNetConfiguration.java:435)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.<clinit>(NeuralNetConfiguration.java:122)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration$Builder.build(NeuralNetConfiguration.java:1045)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration$ListBuilder.build(NeuralNetConfiguration.java:290)\n\tat org.deeplearning4j.nn.modelimport.keras.KerasSequentialModel.getMultiLayerConfiguration(KerasSequentialModel.java:203)\n\tat org.deeplearning4j.nn.modelimport.keras.KerasSequentialModel.getMultiLayerNetwork(KerasSequentialModel.java:223)\n\tat org.deeplearning4j.nn.modelimport.keras.KerasSequentialModel.getMultiLayerNetwork(KerasSequentialModel.java:213)\n\tat org.deeplearning4j.nn.modelimport.keras.KerasModelImport.importKerasSequentialModelAndWeights(KerasModelImport.java:143)\n\tat skymind.dsx.KerasImportCSVSparkRunner.entryPoint(KerasImportCSVSparkRunner.java:102)\n\tat skymind.dsx.KerasImportCSVSparkRunner.main(KerasImportCSVSparkRunner.java:80)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\n\tat java.lang.reflect.Method.invoke(Method.java:507)\n\tat org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:819)\n\tat org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:196)\n\tat org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:221)\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:129)\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n18/04/30 00:58:34 WARN org.reflections.Reflections: could not create Vfs.Dir from url. ignoring the exception and continuing\norg.reflections.ReflectionsException: could not create Vfs.Dir from url, no matching UrlType was found [file:/usr/local/src/dataconnector-dw-2.0/spark-2.0.0/Server/connectivity/connectors/*/*]\neither use fromURL(final URL url, final List<UrlType> urlTypes) or use the static setDefaultURLTypes(final List<UrlType> urlTypes) or addDefaultURLTypes(UrlType urlType) with your specialized UrlType.\n\tat org.reflections.vfs.Vfs.fromURL(Vfs.java:109)\n\tat org.reflections.vfs.Vfs.fromURL(Vfs.java:91)\n\tat org.reflections.Reflections.scan(Reflections.java:237)\n\tat org.reflections.Reflections.scan(Reflections.java:204)\n\tat org.reflections.Reflections.<init>(Reflections.java:129)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.registerSubtypes(NeuralNetConfiguration.java:507)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.configureMapper(NeuralNetConfiguration.java:462)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.initMapper(NeuralNetConfiguration.java:435)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.<clinit>(NeuralNetConfiguration.java:122)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration$Builder.build(NeuralNetConfiguration.java:1045)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration$ListBuilder.build(NeuralNetConfiguration.java:290)\n\tat org.deeplearning4j.nn.modelimport.keras.KerasSequentialModel.getMultiLayerConfiguration(KerasSequentialModel.java:203)\n\tat org.deeplearning4j.nn.modelimport.keras.KerasSequentialModel.getMultiLayerNetwork(KerasSequentialModel.java:223)\n\tat org.deeplearning4j.nn.modelimport.keras.KerasSequentialModel.getMultiLayerNetwork(KerasSequentialModel.java:213)\n\tat org.deeplearning4j.nn.modelimport.keras.KerasModelImport.importKerasSequentialModelAndWeights(KerasModelImport.java:143)\n\tat skymind.dsx.KerasImportCSVSparkRunner.entryPoint(KerasImportCSVSparkRunner.java:102)\n\tat skymind.dsx.KerasImportCSVSparkRunner.main(KerasImportCSVSparkRunner.java:80)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\n\tat java.lang.reflect.Method.invoke(Method.java:507)\n\tat org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:819)\n\tat org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:196)\n\tat org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:221)\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:129)\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n18/04/30 00:58:38 WARN org.reflections.Reflections: could not create Dir using directory from url file:/usr/local/src/dataconnector-dw-2.0/spark-2.0.0/Server/connectivity/jdbc/lib/*. skipping.\njava.lang.NullPointerException\n\tat org.reflections.vfs.Vfs$DefaultUrlTypes$3.matches(Vfs.java:239)\n\tat org.reflections.vfs.Vfs.fromURL(Vfs.java:98)\n\tat org.reflections.vfs.Vfs.fromURL(Vfs.java:91)\n\tat org.reflections.Reflections.scan(Reflections.java:237)\n\tat org.reflections.Reflections.scan(Reflections.java:204)\n\tat org.reflections.Reflections.<init>(Reflections.java:129)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.registerSubtypes(NeuralNetConfiguration.java:507)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.configureMapper(NeuralNetConfiguration.java:462)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.initMapper(NeuralNetConfiguration.java:435)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.<clinit>(NeuralNetConfiguration.java:122)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration$Builder.build(NeuralNetConfiguration.java:1045)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration$ListBuilder.build(NeuralNetConfiguration.java:290)\n\tat org.deeplearning4j.nn.modelimport.keras.KerasSequentialModel.getMultiLayerConfiguration(KerasSequentialModel.java:203)\n\tat org.deeplearning4j.nn.modelimport.keras.KerasSequentialModel.getMultiLayerNetwork(KerasSequentialModel.java:223)\n\tat org.deeplearning4j.nn.modelimport.keras.KerasSequentialModel.getMultiLayerNetwork(KerasSequentialModel.java:213)\n\tat org.deeplearning4j.nn.modelimport.keras.KerasModelImport.importKerasSequentialModelAndWeights(KerasModelImport.java:143)\n\tat skymind.dsx.KerasImportCSVSparkRunner.entryPoint(KerasImportCSVSparkRunner.java:102)\n\tat skymind.dsx.KerasImportCSVSparkRunner.main(KerasImportCSVSparkRunner.java:80)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\n\tat java.lang.reflect.Method.invoke(Method.java:507)\n\tat org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:819)\n\tat org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:196)\n\tat org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:221)\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:129)\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n18/04/30 00:58:38 WARN org.reflections.Reflections: could not create Vfs.Dir from url. ignoring the exception and continuing\norg.reflections.ReflectionsException: could not create Vfs.Dir from url, no matching UrlType was found [file:/usr/local/src/dataconnector-dw-2.0/spark-2.0.0/Server/connectivity/jdbc/lib/*]\neither use fromURL(final URL url, final List<UrlType> urlTypes) or use the static setDefaultURLTypes(final List<UrlType> urlTypes) or addDefaultURLTypes(UrlType urlType) with your specialized UrlType.\n\tat org.reflections.vfs.Vfs.fromURL(Vfs.java:109)\n\tat org.reflections.vfs.Vfs.fromURL(Vfs.java:91)\n\tat org.reflections.Reflections.scan(Reflections.java:237)\n\tat org.reflections.Reflections.scan(Reflections.java:204)\n\tat org.reflections.Reflections.<init>(Reflections.java:129)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.registerSubtypes(NeuralNetConfiguration.java:507)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.configureMapper(NeuralNetConfiguration.java:462)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.initMapper(NeuralNetConfiguration.java:435)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.<clinit>(NeuralNetConfiguration.java:122)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration$Builder.build(NeuralNetConfiguration.java:1045)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration$ListBuilder.build(NeuralNetConfiguration.java:290)\n\tat org.deeplearning4j.nn.modelimport.keras.KerasSequentialModel.getMultiLayerConfiguration(KerasSequentialModel.java:203)\n\tat org.deeplearning4j.nn.modelimport.keras.KerasSequentialModel.getMultiLayerNetwork(KerasSequentialModel.java:223)\n\tat org.deeplearning4j.nn.modelimport.keras.KerasSequentialModel.getMultiLayerNetwork(KerasSequentialModel.java:213)\n\tat org.deeplearning4j.nn.modelimport.keras.KerasModelImport.importKerasSequentialModelAndWeights(KerasModelImport.java:143)\n\tat skymind.dsx.KerasImportCSVSparkRunner.entryPoint(KerasImportCSVSparkRunner.java:102)\n\tat skymind.dsx.KerasImportCSVSparkRunner.main(KerasImportCSVSparkRunner.java:80)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\n\tat java.lang.reflect.Method.invoke(Method.java:507)\n\tat org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:819)\n\tat org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:196)\n\tat org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:221)\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:129)\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n"
                }, 
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "18/04/30 00:58:48 WARN org.reflections.Reflections: could not create Dir using directory from url file:/usr/local/src/dataconnector-dw-2.0/spark-2.0.0/ASBServer/apps/lib/iis/*/*. skipping.\r\njava.lang.NullPointerException\r\n\tat org.reflections.vfs.Vfs$DefaultUrlTypes$3.matches(Vfs.java:239)\r\n\tat org.reflections.vfs.Vfs.fromURL(Vfs.java:98)\r\n\tat org.reflections.vfs.Vfs.fromURL(Vfs.java:91)\r\n\tat org.reflections.Reflections.scan(Reflections.java:237)\r\n\tat org.reflections.Reflections.scan(Reflections.java:204)\r\n\tat org.reflections.Reflections.<init>(Reflections.java:129)\r\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.registerSubtypes(NeuralNetConfiguration.java:507)\r\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.configureMapper(NeuralNetConfiguration.java:462)\r\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.initMapper(NeuralNetConfiguration.java:435)\r\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.<clinit>(NeuralNetConfiguration.java:122)\r\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration$Builder.build(NeuralNetConfiguration.java:1045)\r\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration$ListBuilder.build(NeuralNetConfiguration.java:290)\r\n\tat org.deeplearning4j.nn.modelimport.keras.KerasSequentialModel.getMultiLayerConfiguration(KerasSequentialModel.java:203)\r\n\tat org.deeplearning4j.nn.modelimport.keras.KerasSequentialModel.getMultiLayerNetwork(KerasSequentialModel.java:223)\r\n\tat org.deeplearning4j.nn.modelimport.keras.KerasSequentialModel.getMultiLayerNetwork(KerasSequentialModel.java:213)\r\n\tat org.deeplearning4j.nn.modelimport.keras.KerasModelImport.importKerasSequentialModelAndWeights(KerasModelImport.java:143)\r\n\tat skymind.dsx.KerasImportCSVSparkRunner.entryPoint(KerasImportCSVSparkRunner.java:102)\r\n\tat skymind.dsx.KerasImportCSVSparkRunner.main(KerasImportCSVSparkRunner.java:80)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\r\n\tat java.lang.reflect.Method.invoke(Method.java:507)\r\n\tat org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:819)\r\n\tat org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:196)\r\n\tat org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:221)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:129)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\n18/04/30 00:58:48 WARN org.reflections.Reflections: could not create Vfs.Dir from url. ignoring the exception and continuing\r\norg.reflections.ReflectionsException: could not create Vfs.Dir from url, no matching UrlType was found [file:/usr/local/src/dataconnector-dw-2.0/spark-2.0.0/ASBServer/apps/lib/iis/*/*]\r\neither use fromURL(final URL url, final List<UrlType> urlTypes) or use the static setDefaultURLTypes(final List<UrlType> urlTypes) or addDefaultURLTypes(UrlType urlType) with your specialized UrlType.\r\n\tat org.reflections.vfs.Vfs.fromURL(Vfs.java:109)\r\n\tat org.reflections.vfs.Vfs.fromURL(Vfs.java:91)\r\n\tat org.reflections.Reflections.scan(Reflections.java:237)\r\n\tat org.reflections.Reflections.scan(Reflections.java:204)\r\n\tat org.reflections.Reflections.<init>(Reflections.java:129)\r\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.registerSubtypes(NeuralNetConfiguration.java:507)\r\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.configureMapper(NeuralNetConfiguration.java:462)\r\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.initMapper(NeuralNetConfiguration.java:435)\r\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.<clinit>(NeuralNetConfiguration.java:122)\r\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration$Builder.build(NeuralNetConfiguration.java:1045)\r\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration$ListBuilder.build(NeuralNetConfiguration.java:290)\r\n\tat org.deeplearning4j.nn.modelimport.keras.KerasSequentialModel.getMultiLayerConfiguration(KerasSequentialModel.java:203)\r\n\tat org.deeplearning4j.nn.modelimport.keras.KerasSequentialModel.getMultiLayerNetwork(KerasSequentialModel.java:223)\r\n\tat org.deeplearning4j.nn.modelimport.keras.KerasSequentialModel.getMultiLayerNetwork(KerasSequentialModel.java:213)\r\n\tat org.deeplearning4j.nn.modelimport.keras.KerasModelImport.importKerasSequentialModelAndWeights(KerasModelImport.java:143)\r\n\tat skymind.dsx.KerasImportCSVSparkRunner.entryPoint(KerasImportCSVSparkRunner.java:102)\r\n\tat skymind.dsx.KerasImportCSVSparkRunner.main(KerasImportCSVSparkRunner.java:80)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\r\n\tat java.lang.reflect.Method.invoke(Method.java:507)\r\n\tat org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:819)\r\n\tat org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:196)\r\n\tat org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:221)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:129)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\n18/04/30 00:58:48 WARN org.reflections.Reflections: could not create Dir using directory from url file:/usr/local/src/dataconnector-dw-2.0/spark-2.0.0/Server/connectivity/jars/JISPlugins/*. skipping.\r\njava.lang.NullPointerException\r\n\tat org.reflections.vfs.Vfs$DefaultUrlTypes$3.matches(Vfs.java:239)\r\n\tat org.reflections.vfs.Vfs.fromURL(Vfs.java:98)\r\n\tat org.reflections.vfs.Vfs.fromURL(Vfs.java:91)\r\n\tat org.reflections.Reflections.scan(Reflections.java:237)\r\n\tat org.reflections.Reflections.scan(Reflections.java:204)\r\n\tat org.reflections.Reflections.<init>(Reflections.java:129)\r\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.registerSubtypes(NeuralNetConfiguration.java:507)\r\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.configureMapper(NeuralNetConfiguration.java:462)\r\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.initMapper(NeuralNetConfiguration.java:435)\r\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.<clinit>(NeuralNetConfiguration.java:122)\r\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration$Builder.build(NeuralNetConfiguration.java:1045)\r\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration$ListBuilder.build(NeuralNetConfiguration.java:290)\r\n\tat org.deeplearning4j.nn.modelimport.keras.KerasSequentialModel.getMultiLayerConfiguration(KerasSequentialModel.java:203)\r\n\tat org.deeplearning4j.nn.modelimport.keras.KerasSequentialModel.getMultiLayerNetwork(KerasSequentialModel.java:223)\r\n\tat org.deeplearning4j.nn.modelimport.keras.KerasSequentialModel.getMultiLayerNetwork(KerasSequentialModel.java:213)\r\n\tat org.deeplearning4j.nn.modelimport.keras.KerasModelImport.importKerasSequentialModelAndWeights(KerasModelImport.java:143)\r\n\tat skymind.dsx.KerasImportCSVSparkRunner.entryPoint(KerasImportCSVSparkRunner.java:102)\r\n\tat skymind.dsx.KerasImportCSVSparkRunner.main(KerasImportCSVSparkRunner.java:80)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\r\n\tat java.lang.reflect.Method.invoke(Method.java:507)\r\n\tat org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:819)\r\n\tat org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:196)\r\n\tat org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:221)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:129)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\n18/04/30 00:58:48 WARN org.reflections.Reflections: could not create Vfs.Dir from url. ignoring the exception and continuing\r\norg.reflections.ReflectionsException: could not create Vfs.Dir from url, no matching UrlType was found [file:/usr/local/src/dataconnector-dw-2.0/spark-2.0.0/Server/connectivity/jars/JISPlugins/*]\r\neither use fromURL(final URL url, final List<UrlType> urlTypes) or use the static setDefaultURLTypes(final List<UrlType> urlTypes) or addDefaultURLTypes(UrlType urlType) with your specialized UrlType.\r\n\tat org.reflections.vfs.Vfs.fromURL(Vfs.java:109)\r\n\tat org.reflections.vfs.Vfs.fromURL(Vfs.java:91)\r\n\tat org.reflections.Reflections.scan(Reflections.java:237)\r\n\tat org.reflections.Reflections.scan(Reflections.java:204)\r\n\tat org.reflections.Reflections.<init>(Reflections.java:129)\r\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.registerSubtypes(NeuralNetConfiguration.java:507)\r\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.configureMapper(NeuralNetConfiguration.java:462)\r\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.initMapper(NeuralNetConfiguration.java:435)\r\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.<clinit>(NeuralNetConfiguration.java:122)\r\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration$Builder.build(NeuralNetConfiguration.java:1045)\r\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration$ListBuilder.build(NeuralNetConfiguration.java:290)\r\n\tat org.deeplearning4j.nn.modelimport.keras.KerasSequentialModel.getMultiLayerConfiguration(KerasSequentialModel.java:203)\r\n\tat org.deeplearning4j.nn.modelimport.keras.KerasSequentialModel.getMultiLayerNetwork(KerasSequentialModel.java:223)\r\n\tat org.deeplearning4j.nn.modelimport.keras.KerasSequentialModel.getMultiLayerNetwork(KerasSequentialModel.java:213)\r\n\tat org.deeplearning4j.nn.modelimport.keras.KerasModelImport.importKerasSequentialModelAndWeights(KerasModelImport.java:143)\r\n\tat skymind.dsx.KerasImportCSVSparkRunner.entryPoint(KerasImportCSVSparkRunner.java:102)\r\n\tat skymind.dsx.KerasImportCSVSparkRunner.main(KerasImportCSVSparkRunner.java:80)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\r\n\tat java.lang.reflect.Method.invoke(Method.java:507)\r\n\tat org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:819)\r\n\tat org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:196)\r\n\tat org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:221)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:129)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\n"
                }, 
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "18/04/30 00:58:50 WARN org.reflections.Reflections: could not create Dir using directory from url file:/usr/local/src/dataconnector-dw-2.0/spark-2.0.0/Server/connectivity/branded_jdbc/lib/*. skipping.\njava.lang.NullPointerException\n\tat org.reflections.vfs.Vfs$DefaultUrlTypes$3.matches(Vfs.java:239)\n\tat org.reflections.vfs.Vfs.fromURL(Vfs.java:98)\n\tat org.reflections.vfs.Vfs.fromURL(Vfs.java:91)\n\tat org.reflections.Reflections.scan(Reflections.java:237)\n\tat org.reflections.Reflections.scan(Reflections.java:204)\n\tat org.reflections.Reflections.<init>(Reflections.java:129)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.registerSubtypes(NeuralNetConfiguration.java:507)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.configureMapper(NeuralNetConfiguration.java:462)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.initMapper(NeuralNetConfiguration.java:435)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.<clinit>(NeuralNetConfiguration.java:122)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration$Builder.build(NeuralNetConfiguration.java:1045)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration$ListBuilder.build(NeuralNetConfiguration.java:290)\n\tat org.deeplearning4j.nn.modelimport.keras.KerasSequentialModel.getMultiLayerConfiguration(KerasSequentialModel.java:203)\n\tat org.deeplearning4j.nn.modelimport.keras.KerasSequentialModel.getMultiLayerNetwork(KerasSequentialModel.java:223)\n\tat org.deeplearning4j.nn.modelimport.keras.KerasSequentialModel.getMultiLayerNetwork(KerasSequentialModel.java:213)\n\tat org.deeplearning4j.nn.modelimport.keras.KerasModelImport.importKerasSequentialModelAndWeights(KerasModelImport.java:143)\n\tat skymind.dsx.KerasImportCSVSparkRunner.entryPoint(KerasImportCSVSparkRunner.java:102)\n\tat skymind.dsx.KerasImportCSVSparkRunner.main(KerasImportCSVSparkRunner.java:80)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\n\tat java.lang.reflect.Method.invoke(Method.java:507)\n\tat org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:819)\n\tat org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:196)\n\tat org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:221)\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:129)\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n18/04/30 00:58:50 WARN org.reflections.Reflections: could not create Vfs.Dir from url. ignoring the exception and continuing\norg.reflections.ReflectionsException: could not create Vfs.Dir from url, no matching UrlType was found [file:/usr/local/src/dataconnector-dw-2.0/spark-2.0.0/Server/connectivity/branded_jdbc/lib/*]\neither use fromURL(final URL url, final List<UrlType> urlTypes) or use the static setDefaultURLTypes(final List<UrlType> urlTypes) or addDefaultURLTypes(UrlType urlType) with your specialized UrlType.\n\tat org.reflections.vfs.Vfs.fromURL(Vfs.java:109)\n\tat org.reflections.vfs.Vfs.fromURL(Vfs.java:91)\n\tat org.reflections.Reflections.scan(Reflections.java:237)\n\tat org.reflections.Reflections.scan(Reflections.java:204)\n\tat org.reflections.Reflections.<init>(Reflections.java:129)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.registerSubtypes(NeuralNetConfiguration.java:507)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.configureMapper(NeuralNetConfiguration.java:462)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.initMapper(NeuralNetConfiguration.java:435)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.<clinit>(NeuralNetConfiguration.java:122)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration$Builder.build(NeuralNetConfiguration.java:1045)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration$ListBuilder.build(NeuralNetConfiguration.java:290)\n\tat org.deeplearning4j.nn.modelimport.keras.KerasSequentialModel.getMultiLayerConfiguration(KerasSequentialModel.java:203)\n\tat org.deeplearning4j.nn.modelimport.keras.KerasSequentialModel.getMultiLayerNetwork(KerasSequentialModel.java:223)\n\tat org.deeplearning4j.nn.modelimport.keras.KerasSequentialModel.getMultiLayerNetwork(KerasSequentialModel.java:213)\n\tat org.deeplearning4j.nn.modelimport.keras.KerasModelImport.importKerasSequentialModelAndWeights(KerasModelImport.java:143)\n\tat skymind.dsx.KerasImportCSVSparkRunner.entryPoint(KerasImportCSVSparkRunner.java:102)\n\tat skymind.dsx.KerasImportCSVSparkRunner.main(KerasImportCSVSparkRunner.java:80)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\n\tat java.lang.reflect.Method.invoke(Method.java:507)\n\tat org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:819)\n\tat org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:196)\n\tat org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:221)\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:129)\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n18/04/30 00:58:51 WARN org.reflections.Reflections: could not create Dir using directory from url file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s613-ce8dfc36d194a2-56b67737d6cd/data/libs/scala-2.11/*. skipping.\njava.lang.NullPointerException\n\tat org.reflections.vfs.Vfs$DefaultUrlTypes$3.matches(Vfs.java:239)\n\tat org.reflections.vfs.Vfs.fromURL(Vfs.java:98)\n\tat org.reflections.vfs.Vfs.fromURL(Vfs.java:91)\n\tat org.reflections.Reflections.scan(Reflections.java:237)\n\tat org.reflections.Reflections.scan(Reflections.java:204)\n\tat org.reflections.Reflections.<init>(Reflections.java:129)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.registerSubtypes(NeuralNetConfiguration.java:507)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.configureMapper(NeuralNetConfiguration.java:462)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.initMapper(NeuralNetConfiguration.java:435)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.<clinit>(NeuralNetConfiguration.java:122)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration$Builder.build(NeuralNetConfiguration.java:1045)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration$ListBuilder.build(NeuralNetConfiguration.java:290)\n\tat org.deeplearning4j.nn.modelimport.keras.KerasSequentialModel.getMultiLayerConfiguration(KerasSequentialModel.java:203)\n\tat org.deeplearning4j.nn.modelimport.keras.KerasSequentialModel.getMultiLayerNetwork(KerasSequentialModel.java:223)\n\tat org.deeplearning4j.nn.modelimport.keras.KerasSequentialModel.getMultiLayerNetwork(KerasSequentialModel.java:213)\n\tat org.deeplearning4j.nn.modelimport.keras.KerasModelImport.importKerasSequentialModelAndWeights(KerasModelImport.java:143)\n\tat skymind.dsx.KerasImportCSVSparkRunner.entryPoint(KerasImportCSVSparkRunner.java:102)\n\tat skymind.dsx.KerasImportCSVSparkRunner.main(KerasImportCSVSparkRunner.java:80)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\n\tat java.lang.reflect.Method.invoke(Method.java:507)\n\tat org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:819)\n\tat org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:196)\n\tat org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:221)\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:129)\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n18/04/30 00:58:51 WARN org.reflections.Reflections: could not create Vfs.Dir from url. ignoring the exception and continuing\norg.reflections.ReflectionsException: could not create Vfs.Dir from url, no matching UrlType was found [file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s613-ce8dfc36d194a2-56b67737d6cd/data/libs/scala-2.11/*]\neither use fromURL(final URL url, final List<UrlType> urlTypes) or use the static setDefaultURLTypes(final List<UrlType> urlTypes) or addDefaultURLTypes(UrlType urlType) with your specialized UrlType.\n\tat org.reflections.vfs.Vfs.fromURL(Vfs.java:109)\n\tat org.reflections.vfs.Vfs.fromURL(Vfs.java:91)\n\tat org.reflections.Reflections.scan(Reflections.java:237)\n\tat org.reflections.Reflections.scan(Reflections.java:204)\n\tat org.reflections.Reflections.<init>(Reflections.java:129)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.registerSubtypes(NeuralNetConfiguration.java:507)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.configureMapper(NeuralNetConfiguration.java:462)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.initMapper(NeuralNetConfiguration.java:435)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration.<clinit>(NeuralNetConfiguration.java:122)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration$Builder.build(NeuralNetConfiguration.java:1045)\n\tat org.deeplearning4j.nn.conf.NeuralNetConfiguration$ListBuilder.build(NeuralNetConfiguration.java:290)\n\tat org.deeplearning4j.nn.modelimport.keras.KerasSequentialModel.getMultiLayerConfiguration(KerasSequentialModel.java:203)\n\tat org.deeplearning4j.nn.modelimport.keras.KerasSequentialModel.getMultiLayerNetwork(KerasSequentialModel.java:223)\n\tat org.deeplearning4j.nn.modelimport.keras.KerasSequentialModel.getMultiLayerNetwork(KerasSequentialModel.java:213)\n\tat org.deeplearning4j.nn.modelimport.keras.KerasModelImport.importKerasSequentialModelAndWeights(KerasModelImport.java:143)\n\tat skymind.dsx.KerasImportCSVSparkRunner.entryPoint(KerasImportCSVSparkRunner.java:102)\n\tat skymind.dsx.KerasImportCSVSparkRunner.main(KerasImportCSVSparkRunner.java:80)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\n\tat java.lang.reflect.Method.invoke(Method.java:507)\n\tat org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:819)\n\tat org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:196)\n\tat org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:221)\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:129)\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n"
                }, 
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "18/04/30 00:58:52 INFO org.reflections.Reflections: Reflections took 28184 ms to scan 448 urls, producing 17893 keys and 119565 values \n18/04/30 00:58:53 INFO nn.multilayer.MultiLayerNetwork: Starting MultiLayerNetwork with WorkspaceModes set to [training: NONE; inference: SEPARATE]\n18/04/30 00:58:53 INFO storage.memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 5.8 KB, free 727.2 MB)\n18/04/30 00:58:53 INFO storage.memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1030.0 B, free 727.2 MB)\n18/04/30 00:58:53 INFO spark.storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.143.133.233:34826 (size: 1030.0 B, free: 727.2 MB)\n18/04/30 00:58:53 INFO apache.spark.SparkContext: Created broadcast 0 from broadcast at SparkDl4jMultiLayer.java:597\n18/04/30 00:58:53 INFO storage.memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.6 KB, free 727.2 MB)\n18/04/30 00:58:53 INFO storage.memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 346.0 B, free 727.2 MB)\n18/04/30 00:58:53 INFO spark.storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.143.133.233:34826 (size: 346.0 B, free: 727.2 MB)\n18/04/30 00:58:53 INFO apache.spark.SparkContext: Created broadcast 1 from broadcast at SparkDl4jMultiLayer.java:598\n18/04/30 00:58:53 INFO apache.spark.SparkContext: Starting job: treeAggregate at SparkDl4jMultiLayer.java:600\n18/04/30 00:58:53 INFO spark.scheduler.DAGScheduler: Got job 0 (treeAggregate at SparkDl4jMultiLayer.java:600) with 2 output partitions\n18/04/30 00:58:53 INFO spark.scheduler.DAGScheduler: Final stage: ResultStage 0 (treeAggregate at SparkDl4jMultiLayer.java:600)\n18/04/30 00:58:53 INFO spark.scheduler.DAGScheduler: Parents of final stage: List()\n18/04/30 00:58:53 INFO spark.scheduler.DAGScheduler: Missing parents: List()\n18/04/30 00:58:53 INFO spark.scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at treeAggregate at SparkDl4jMultiLayer.java:600), which has no missing parents\n18/04/30 00:58:54 INFO storage.memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 7.2 KB, free 727.2 MB)\n18/04/30 00:58:54 INFO storage.memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.4 KB, free 727.2 MB)\n18/04/30 00:58:54 INFO spark.storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.143.133.233:34826 (size: 3.4 KB, free: 727.2 MB)\n18/04/30 00:58:54 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(0)\n18/04/30 00:58:54 INFO cluster.ego.EGOFineGrainedSchedulerBackend: RDD 2: CPU\n18/04/30 00:58:54 INFO cluster.ego.EGOFineGrainedSchedulerBackend: RDD 1: CPU\n18/04/30 00:58:54 INFO cluster.ego.EGOFineGrainedSchedulerBackend: RDD 0: CPU\n18/04/30 00:58:54 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId is 0, rg is CPU\n18/04/30 00:58:54 INFO apache.spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:997\n18/04/30 00:58:54 INFO spark.scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at treeAggregate at SparkDl4jMultiLayer.java:600)\n18/04/30 00:58:54 INFO cluster.ego.EGODeployScheduler: Adding task set 0.0 with 2 tasks\n18/04/30 00:58:54 INFO cluster.ego.EGOFineGrainedSchedulerBackend: <EVENT> Spark driver SPARKDRIVER:d68bf52a-2964-4f50-a6a2-682cbf130af3 workload coming in\n18/04/30 00:58:59 INFO cluster.ego.EGOFineGrainedSchedulerBackend: Registered executor NettyRpcEndpointRef(null) (10.142.18.236:39390) with ID c82b0e50-51ef-45b5-b9e9-e654b43078df\n18/04/30 00:58:59 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, yp-spark-dal09-env5-0021, executor c82b0e50-51ef-45b5-b9e9-e654b43078df, partition 0, PROCESS_LOCAL, 9379 bytes)\n18/04/30 00:58:59 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onTaskStart: TID 0 ( Index 0 ) on c82b0e50-51ef-45b5-b9e9-e654b43078df\n18/04/30 00:58:59 INFO spark.storage.BlockManagerMasterEndpoint: Registering block manager yp-spark-dal09-env5-0021:45594 with 3.4 GB RAM, BlockManagerId(c82b0e50-51ef-45b5-b9e9-e654b43078df, yp-spark-dal09-env5-0021, 45594, None)\n18/04/30 00:59:00 INFO cluster.ego.EGOFineGrainedSchedulerBackend: Registered executor NettyRpcEndpointRef(null) (10.143.133.40:58498) with ID d1029f91-78f9-48d1-bbf8-e95760963399\n18/04/30 00:59:00 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, yp-spark-dal09-env5-0045, executor d1029f91-78f9-48d1-bbf8-e95760963399, partition 1, PROCESS_LOCAL, 9379 bytes)\n18/04/30 00:59:00 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onTaskStart: TID 1 ( Index 1 ) on d1029f91-78f9-48d1-bbf8-e95760963399\n18/04/30 00:59:00 INFO spark.storage.BlockManagerMasterEndpoint: Registering block manager yp-spark-dal09-env5-0045:39002 with 3.4 GB RAM, BlockManagerId(d1029f91-78f9-48d1-bbf8-e95760963399, yp-spark-dal09-env5-0045, 39002, None)\n18/04/30 00:59:33 INFO spark.storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on yp-spark-dal09-env5-0045:39002 (size: 3.4 KB, free: 3.4 GB)\n18/04/30 00:59:33 INFO spark.storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on yp-spark-dal09-env5-0021:45594 (size: 3.4 KB, free: 3.4 GB)\n18/04/30 00:59:33 INFO spark.storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on yp-spark-dal09-env5-0045:39002 (size: 346.0 B, free: 3.4 GB)\n18/04/30 00:59:33 INFO spark.storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on yp-spark-dal09-env5-0045:39002 (size: 1030.0 B, free: 3.4 GB)\n18/04/30 00:59:33 INFO spark.storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on yp-spark-dal09-env5-0021:45594 (size: 346.0 B, free: 3.4 GB)\n18/04/30 00:59:33 INFO spark.storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on yp-spark-dal09-env5-0021:45594 (size: 1030.0 B, free: 3.4 GB)\n18/04/30 01:00:02 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 62045 ms on yp-spark-dal09-env5-0045 (executor d1029f91-78f9-48d1-bbf8-e95760963399) (1/2)\n18/04/30 01:00:02 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onTaskEnd: TID 1 ( Index 1 ) on d1029f91-78f9-48d1-bbf8-e95760963399 with SUCCESS\n18/04/30 01:00:05 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 65597 ms on yp-spark-dal09-env5-0021 (executor c82b0e50-51ef-45b5-b9e9-e654b43078df) (2/2)\n18/04/30 01:00:05 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onTaskEnd: TID 0 ( Index 0 ) on c82b0e50-51ef-45b5-b9e9-e654b43078df with SUCCESS\n18/04/30 01:00:05 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \n18/04/30 01:00:05 INFO spark.scheduler.DAGScheduler: ResultStage 0 (treeAggregate at SparkDl4jMultiLayer.java:600) finished in 71.504 s\n18/04/30 01:00:05 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(0)\n18/04/30 01:00:05 INFO spark.scheduler.DAGScheduler: Job 0 finished: treeAggregate at SparkDl4jMultiLayer.java:600, took 71.631759 s\n18/04/30 01:00:05 INFO skymind.dsx.KerasImportCSVSparkRunner: \nExamples labeled as 0 classified by model as 1: 50 times\nExamples labeled as 1 classified by model as 1: 50 times\nExamples labeled as 2 classified by model as 1: 50 times\n\nWarning: 2 classes were never predicted by the model and were excluded from average precision\nClasses excluded from average precision: [0, 2]\n\n==========================Scores========================================\n # of classes:    3\n Accuracy:        0.3333\n Precision:       0.3333\t(2 classes excluded from average)\n Recall:          0.3333\n F1 Score:        0.5000\t(2 classes excluded from average)\nPrecision, recall & F1: macro-averaged (equally weighted avg. of 3 classes)\n========================================================================\n18/04/30 01:00:05 INFO skymind.dsx.KerasImportCSVSparkRunner: ***** Example Complete *****\n18/04/30 01:00:05 INFO apache.spark.SparkContext: Invoking stop() from shutdown hook\n18/04/30 01:00:05 INFO cluster.ego.EGOFineGrainedSchedulerBackend: hosts Set(yp-spark-dal09-env5-0021, yp-spark-dal09-env5-0045) need to delete the cache data for application[%s] app-20180430005822-0082-f163ea89-e12a-495a-ae74-10361283fcb5\n18/04/30 01:00:05 INFO network.client.TransportClientFactory: Successfully created connection to yp-spark-dal09-env5-0021/10.142.18.236:7342 after 6 ms (6 ms spent in bootstraps)\n18/04/30 01:00:05 INFO network.client.TransportClientFactory: Successfully created connection to yp-spark-dal09-env5-0045/10.143.133.40:7342 after 6 ms (5 ms spent in bootstraps)\n18/04/30 01:00:05 INFO cluster.ego.EGOFineGrainedSchedulerBackend: Waiting for rpc request receive the response.\n"
                }, 
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "18/04/30 01:00:06 INFO cluster.ego.EGOFineGrainedSchedulerBackend: Spark driver enters idle mode\n18/04/30 01:00:06 INFO cluster.ego.EGODeployScheduler: Spark context stopped.\n18/04/30 01:00:06 INFO apache.spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n18/04/30 01:00:06 INFO storage.memory.MemoryStore: MemoryStore cleared\n18/04/30 01:00:06 INFO spark.storage.BlockManager: BlockManager stopped\n18/04/30 01:00:06 INFO spark.storage.BlockManagerMaster: BlockManagerMaster stopped\n18/04/30 01:00:06 WARN rpc.netty.Dispatcher: Message RemoteProcessDisconnected(10.142.18.236:39390) dropped. Could not find BlockManagerMaster.\n18/04/30 01:00:06 INFO spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n18/04/30 01:00:06 INFO apache.spark.SparkContext: Successfully stopped SparkContext\n18/04/30 01:00:06 INFO spark.util.ShutdownHookManager: Shutdown hook called\n18/04/30 01:00:06 INFO spark.util.ShutdownHookManager: Deleting directory /tmp/spark-21-ego-master/work/spark-efda511b-65f4-40ed-b11b-21ccc57aac51\n"
                }
            ], 
            "execution_count": 20
        }, 
        {
            "source": "!mv dl4j-snapshot.jar.1 dl4-snapshot.jar\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "!ls dl4j-snapshot.jar", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "dl4j-snapshot.jar\r\n"
                }
            ], 
            "execution_count": 3
        }, 
        {
            "source": "", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 with Spark 2.1", 
            "name": "python3-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "pygments_lexer": "ipython3", 
            "file_extension": ".py", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}